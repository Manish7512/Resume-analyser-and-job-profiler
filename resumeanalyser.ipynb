{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manish7512/Resume-analyser-and-job-profiler/blob/main/resumeanalyser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "6fjCJx6jVPpc",
        "outputId": "19ca9f98-0b11-4727-9500-8646aa965135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.67)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.14)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Downloading langchain_google_genai-2.1.6-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, python-dotenv, PyPDF2, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-huggingface, google-ai-generativelanguage, langchain_google_genai, langchain-community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 dataclasses-json-0.6.7 faiss-cpu-1.11.0 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-huggingface-0.3.0 langchain_google_genai-2.1.6 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "b20372e6afe74b4f8eee233aaa446336"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain_google_genai langchain-community huggingface_hub PyPDF2 langchain-huggingface faiss-cpu gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b69fb4220b664f63a70ee5140c47ab6b",
            "58dd3835baac4265ac2fef5b080c2506",
            "8901eb8f27084b828ffa88b3d28c6d39",
            "578c555100314f52a02011cfbf3623a4",
            "21aa1b56fccc430ab51ba95adc63b5d7",
            "111589b690f34fc2bf17424b398c0cd0",
            "6b013db7a5d74bc9a10c754046c4fbab",
            "f9fa9c40a57f4955ae7052d5bce24c2f",
            "201de180fdca4591872827b8f3ba1418",
            "4fb25c6acf5a4953a665d2bfe98e03f7",
            "d1fba7cd5c904f079f3ace2ff075a977",
            "80425431d76c4ce4a73029345f5d64e6",
            "5d5610a95cc341759e30af51ba09f5a6",
            "ee6c13b39c6541548b6b5d34657262d2"
          ]
        },
        "id": "i9wP9T1sUYFx",
        "outputId": "60e19b78-978e-499e-a6c6-6714317e77d4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2631737058.py:401: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://685210afc1359e0d54.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://685210afc1359e0d54.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b69fb4220b664f63a70ee5140c47ab6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58dd3835baac4265ac2fef5b080c2506",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8901eb8f27084b828ffa88b3d28c6d39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "578c555100314f52a02011cfbf3623a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21aa1b56fccc430ab51ba95adc63b5d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "111589b690f34fc2bf17424b398c0cd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b013db7a5d74bc9a10c754046c4fbab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9fa9c40a57f4955ae7052d5bce24c2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "201de180fdca4591872827b8f3ba1418",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fb25c6acf5a4953a665d2bfe98e03f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1fba7cd5c904f079f3ace2ff075a977",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80425431d76c4ce4a73029345f5d64e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d5610a95cc341759e30af51ba09f5a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee6c13b39c6541548b6b5d34657262d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2631737058.py:100: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "# import tempfile\n",
        "# import shutil\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load environment variables\n",
        "# load_dotenv()\n",
        "\n",
        "class ResumeAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.vectorstore = None\n",
        "        self.conversation_chain = None\n",
        "        self.memory = None\n",
        "        self.processed_files = []\n",
        "\n",
        "    def extract_pdf_text(self, pdf_files):\n",
        "        \"\"\"Extract text from uploaded PDF files\"\"\"\n",
        "        if not pdf_files:\n",
        "            return \"\"\n",
        "\n",
        "        text = \"\"\n",
        "        self.processed_files = []\n",
        "\n",
        "        for pdf_file in pdf_files:\n",
        "            try:\n",
        "                # Handle file path (Gradio returns file paths as strings)\n",
        "                pdf_path = pdf_file if isinstance(pdf_file, str) else pdf_file.name\n",
        "                self.processed_files.append(os.path.basename(pdf_path))\n",
        "\n",
        "                pdf_reader = PdfReader(pdf_path)\n",
        "                for page in pdf_reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return text\n",
        "\n",
        "    def create_text_chunks(self, text):\n",
        "        \"\"\"Split text into chunks for processing\"\"\"\n",
        "        if not text.strip():\n",
        "            return []\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(\n",
        "            separator=\"\\n\",\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.split_text(text)\n",
        "        return chunks\n",
        "\n",
        "    def create_vectorstore(self, text_chunks):\n",
        "        \"\"\"Create FAISS vector store from text chunks\"\"\"\n",
        "        if not text_chunks:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            embeddings = HuggingFaceEmbeddings(\n",
        "                model_name=\"hkunlp/instructor-xl\",\n",
        "                # model_kwargs={\"device\": \"cpu\"}\n",
        "                model_kwargs={\"device\": \"cuda\"}\n",
        "            )\n",
        "\n",
        "            vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "            return vectorstore\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating vectorstore: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def setup_conversation_chain(self, vectorstore):\n",
        "        \"\"\"Setup the conversation chain with Gemini LLM\"\"\"\n",
        "        if not vectorstore:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Initialize Gemini\n",
        "            llm = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-2.5-flash\",  # Updated model name\n",
        "                temperature=0.7,\n",
        "            )\n",
        "\n",
        "            # Memory\n",
        "            self.memory = ConversationBufferMemory(\n",
        "                memory_key=\"chat_history\",\n",
        "                return_messages=True\n",
        "            )\n",
        "\n",
        "            # Prompt template\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", \"\"\"You are an AI assistant for a resume analyzer system.\n",
        "You MUST ONLY answer questions related to resume analysis, job profiling, and candidate evaluation based on the uploaded resumes.\n",
        "\n",
        "STRICT RULES:\n",
        "1. ONLY respond to queries about:\n",
        "   - Finding candidates for specific job roles\n",
        "   - Analyzing skills and qualifications from resumes\n",
        "   - Comparing candidates for positions\n",
        "   - Extracting contact information from resumes\n",
        "   - Summarizing candidate profiles\n",
        "   - Job-related questions about the uploaded resumes\n",
        "\n",
        "2. If asked about ANYTHING else (history, general knowledge, unrelated topics, etc.), respond with:\n",
        "   \"I can only help with resume analysis and job profiling based on the uploaded resumes. Please ask questions about finding candidates, analyzing skills, or job-related queries.\"\n",
        "\n",
        "3. IMPORTANT: If the context shows \"No relevant resume information found\", it means no candidates in the database match the query. In this case, respond with:\n",
        "   \"❌ No candidates found matching your criteria. This could mean:\n",
        "   • No resumes in the database match the specified skills/role\n",
        "   • The job title or skills mentioned aren't present in the uploaded resumes\n",
        "   • Try broadening your search criteria or using different keywords\n",
        "\n",
        "   Consider rephrasing your query or checking if the relevant resumes were properly uploaded.\"\n",
        "\n",
        "4. For valid resume-related queries with relevant context, provide:\n",
        "   - Full name\n",
        "   - Email address (if available)\n",
        "   - LinkedIn profile link (if available)\n",
        "   - Phone number (if available)\n",
        "   - A concise summary of their qualifications and experience\n",
        "   - Key skills that match the job requirements\n",
        "   - Years of experience (if mentioned)\n",
        "\n",
        "5. Present information in a clear, organized format. If contact information is not available, mention \"Not provided\" for those fields.\n",
        "\n",
        "6. Never make up or hallucinate information about candidates. Only use information explicitly provided in the context.\n",
        "\n",
        "Context from uploaded resumes: {context}\"\"\"),\n",
        "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "                (\"human\", \"{question}\")\n",
        "            ])\n",
        "\n",
        "            # Helper functions\n",
        "            def format_docs(docs):\n",
        "                if not docs:\n",
        "                    return \"No relevant resume information found.\"\n",
        "\n",
        "                # Check if documents have meaningful content\n",
        "                meaningful_docs = []\n",
        "                for doc in docs:\n",
        "                    if doc.page_content and len(doc.page_content.strip()) > 10:\n",
        "                        meaningful_docs.append(doc)\n",
        "\n",
        "                if not meaningful_docs:\n",
        "                    return \"No relevant resume information found.\"\n",
        "\n",
        "                return \"\\n\\n\".join(doc.page_content for doc in meaningful_docs)\n",
        "\n",
        "            def get_chat_history(inputs):\n",
        "                return self.memory.chat_memory.messages if self.memory else []\n",
        "\n",
        "            def enhanced_retriever(query):\n",
        "                \"\"\"Enhanced retriever with similarity threshold checking\"\"\"\n",
        "                try:\n",
        "                    # Perform similarity search with scores\n",
        "                    docs_with_scores = vectorstore.similarity_search_with_score(query, k=5)\n",
        "\n",
        "                    # Filter documents based on similarity threshold\n",
        "                    # Lower scores indicate higher similarity in FAISS\n",
        "                    similarity_threshold = 1.5  # Adjust based on your needs\n",
        "\n",
        "                    relevant_docs = []\n",
        "                    for doc, score in docs_with_scores:\n",
        "                        if score < similarity_threshold:  # Lower score = more similar\n",
        "                            relevant_docs.append(doc)\n",
        "\n",
        "                    # If no documents meet the threshold, return empty list\n",
        "                    if not relevant_docs:\n",
        "                        return []\n",
        "\n",
        "                    return relevant_docs\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Retrieval error: {e}\")\n",
        "                    return []\n",
        "\n",
        "            # Create the chain\n",
        "            rag_chain = (\n",
        "                {\n",
        "                    \"context\": RunnableLambda(enhanced_retriever) | format_docs,\n",
        "                    \"question\": RunnablePassthrough(),\n",
        "                    \"chat_history\": RunnableLambda(get_chat_history)\n",
        "                }\n",
        "                | prompt\n",
        "                | llm\n",
        "                | StrOutputParser()\n",
        "            )\n",
        "\n",
        "            # Wrapper to handle memory\n",
        "            def conversation_with_memory(question):\n",
        "                try:\n",
        "                    response = rag_chain.invoke(question)\n",
        "                    # Save to memory\n",
        "                    if self.memory:\n",
        "                        self.memory.chat_memory.add_user_message(question)\n",
        "                        self.memory.chat_memory.add_ai_message(response)\n",
        "                    return response\n",
        "                except Exception as e:\n",
        "                    return f\"Error processing query: {str(e)}\"\n",
        "\n",
        "            return conversation_with_memory\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up conversation chain: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_resumes(self, pdf_files, progress=gr.Progress()):\n",
        "        \"\"\"Process uploaded resume PDFs\"\"\"\n",
        "        if not pdf_files:\n",
        "            return \"❌ No files uploaded. Please upload PDF resumes.\", \"\"\n",
        "\n",
        "        try:\n",
        "            progress(0.1, desc=\"Extracting text from PDFs...\")\n",
        "\n",
        "            # Extract text from PDFs\n",
        "            raw_text = self.extract_pdf_text(pdf_files)\n",
        "\n",
        "            if not raw_text.strip():\n",
        "                return \"❌ No text could be extracted from the uploaded PDFs.\", \"\"\n",
        "\n",
        "            progress(0.3, desc=\"Creating text chunks...\")\n",
        "\n",
        "            # Create text chunks\n",
        "            text_chunks = self.create_text_chunks(raw_text)\n",
        "\n",
        "            if not text_chunks:\n",
        "                return \"❌ Could not create text chunks from the extracted text.\", \"\"\n",
        "\n",
        "            progress(0.6, desc=\"Creating vector database...\")\n",
        "\n",
        "            # Create vector store\n",
        "            self.vectorstore = self.create_vectorstore(text_chunks)\n",
        "\n",
        "            if not self.vectorstore:\n",
        "                return \"❌ Failed to create vector database.\", \"\"\n",
        "\n",
        "            progress(0.8, desc=\"Setting up AI conversation chain...\")\n",
        "\n",
        "            # Setup conversation chain\n",
        "            self.conversation_chain = self.setup_conversation_chain(self.vectorstore)\n",
        "\n",
        "            if not self.conversation_chain:\n",
        "                return \"❌ Failed to setup AI conversation chain.\", \"\"\n",
        "\n",
        "            progress(1.0, desc=\"Processing complete!\")\n",
        "\n",
        "            success_msg = f\"\"\"✅ **Processing Complete!**\n",
        "\n",
        "📄 **Files Processed:** {len(self.processed_files)}\n",
        "📝 **Text Chunks Created:** {len(text_chunks)}\n",
        "🔍 **Vector Database:** Ready\n",
        "🤖 **AI System:** Initialized\n",
        "\n",
        "**Processed Files:**\n",
        "{chr(10).join(f\"• {file}\" for file in self.processed_files)}\n",
        "\n",
        "You can now query for job profiles using the chat interface below.\"\"\"\n",
        "\n",
        "            return success_msg, \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error processing resumes: {str(e)}\", \"\"\n",
        "\n",
        "    def chat_with_system(self, message, history):\n",
        "        \"\"\"Handle chat interactions with context validation\"\"\"\n",
        "        if not self.conversation_chain:\n",
        "            return history + [(message, \"❌ Please upload and process resumes first.\")], \"\"\n",
        "\n",
        "        if not message.strip():\n",
        "            return history, \"\"\n",
        "\n",
        "        # Check if the question is resume/job-related\n",
        "        if not self._is_resume_related_query(message):\n",
        "            response = \"I can only help with resume analysis and job profiling based on the uploaded resumes. Please ask questions about finding candidates, analyzing skills, or job-related queries.\"\n",
        "            history.append((message, response))\n",
        "            return history, \"\"\n",
        "\n",
        "        try:\n",
        "            # Get response from the conversation chain\n",
        "            response = self.conversation_chain(message)\n",
        "\n",
        "            # Update chat history\n",
        "            history.append((message, response))\n",
        "\n",
        "            return history, \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error: {str(e)}\"\n",
        "            history.append((message, error_msg))\n",
        "            return history, \"\"\n",
        "\n",
        "    def _is_resume_related_query(self, query):\n",
        "        \"\"\"Check if the query is related to resume analysis or job profiling\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Keywords that indicate resume/job-related queries\n",
        "        resume_keywords = [\n",
        "            'candidate', 'candidates', 'resume', 'resumes', 'job', 'position', 'role',\n",
        "            'skill', 'skills', 'experience', 'qualification', 'qualifications',\n",
        "            'developer', 'engineer', 'manager', 'analyst', 'designer', 'consultant',\n",
        "            'hire', 'hiring', 'recruit', 'recruitment', 'interview', 'profile',\n",
        "            'background', 'expertise', 'competency', 'competencies', 'ability',\n",
        "            'python', 'java', 'javascript', 'react', 'node', 'sql', 'database',\n",
        "            'frontend','degree', 'certification', 'portfolio', 'github',\n",
        "            'linkedin', 'contact', 'email', 'phone', 'name', 'find', 'search',\n",
        "            'best', 'suitable', 'match', 'fit', 'senior', 'junior', 'entry level',\n",
        "            'years of experience', 'cv', 'curriculum vitae'\n",
        "        ]\n",
        "\n",
        "        # Check if any resume-related keywords are present\n",
        "        return any(keyword in query_lower for keyword in resume_keywords)\n",
        "\n",
        "# Initialize the resume analyzer\n",
        "analyzer = ResumeAnalyzer()\n",
        "\n",
        "# Create Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks(\n",
        "        title=\"Resume Analyzer & Job Profiler\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        css=\"\"\"\n",
        "        .header { text-align: center; margin-bottom: 20px; }\n",
        "        .status-box { padding: 15px; border-radius: 10px; margin: 10px 0; }\n",
        "        .upload-area { border: 2px dashed #ccc; padding: 20px; border-radius: 10px; }\n",
        "        \"\"\"\n",
        "    ) as demo:\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div class=\"header\">\n",
        "            <h1>🎯 Resume Analyzer & Job Profiler</h1>\n",
        "            <p>Upload resumes and find the best candidates for any job profile using AI</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tab(\"📤 Upload & Process Resumes\"):\n",
        "            gr.HTML(\"\"\"\n",
        "            <!-- <div style=\"background: #f0f8ff; padding: 15px; border-radius: 10px; margin-bottom: 20px;\"> -->\n",
        "            <div style=\"padding: 15px; border-radius: 10px; margin-bottom: 20px;\">\n",
        "                <h3>Step 1: Upload Resume PDFs</h3>\n",
        "                <p>Upload multiple PDF resumes to build your candidate database. The system will extract text and create a searchable vector database.</p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=2):\n",
        "                    file_upload = gr.File(\n",
        "                        label=\"Upload Resume PDFs\",\n",
        "                        file_count=\"multiple\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        interactive=True\n",
        "                    )\n",
        "\n",
        "                    process_btn = gr.Button(\n",
        "                        \"🚀 Process Resumes\",\n",
        "                        variant=\"primary\",\n",
        "                        size=\"lg\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=1):\n",
        "                    gr.HTML(\"\"\"\n",
        "                    <!-- <div style=\"background: #fff3cd; padding: 15px; border-radius: 10px;\"> -->\n",
        "                    <div style=\"padding: 15px; border-radius: 10px;\">\n",
        "                        <h4>📋 Requirements</h4>\n",
        "                        <ul>\n",
        "                            <li>PDF format only</li>\n",
        "                            <li>Text-based PDFs (not scanned images)</li>\n",
        "                            <li>Multiple files supported</li>\n",
        "                            <li>Processing may take a few minutes</li>\n",
        "                        </ul>\n",
        "                    </div>\n",
        "                    \"\"\")\n",
        "\n",
        "            status_output = gr.HTML(label=\"Processing Status\")\n",
        "\n",
        "        with gr.Tab(\"💬 Query Candidates\"):\n",
        "            gr.HTML(\"\"\"\n",
        "            <!-- <div style=\"background: #f0fff0; padding: 15px; border-radius: 10px; margin-bottom: 20px;\"> -->\n",
        "            <div style=\"padding: 15px; border-radius: 10px; margin-bottom: 20px;\">\n",
        "                <h3>Step 2: Find the Best Candidates</h3>\n",
        "                <p>Ask questions about job profiles to find the most suitable candidates from your uploaded resumes.</p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"AI Resume Analyzer\",\n",
        "                height=500,\n",
        "                placeholder=\"Process resumes first, then start chatting...\"\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(\n",
        "                    label=\"Your Query\",\n",
        "                    placeholder=\"e.g., 'Who are the best candidates for a senior Python developer position?'\",\n",
        "                    lines=2,\n",
        "                    scale=4\n",
        "                )\n",
        "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    \"Who are the best candidates for a software engineer position?\",\n",
        "                    \"Find candidates with React.js and Node.js experience\",\n",
        "                    \"Show me candidates suitable for a data scientist role\",\n",
        "                    \"Who has the most experience in machine learning?\",\n",
        "                    \"Find candidates with project management experience\",\n",
        "                    \"Show me candidates with both frontend and backend skills\"\n",
        "                ],\n",
        "                inputs=msg_input,\n",
        "                label=\"Example Queries\"\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"ℹ️ About\"):\n",
        "            gr.HTML(\"\"\"\n",
        "            <div style=\"padding: 20px;\">\n",
        "                <h2>About Resume Analyzer & Job Profiler</h2>\n",
        "\n",
        "                <h3>🔧 Technology Stack</h3>\n",
        "                <ul>\n",
        "                    <li><strong>LangChain:</strong> Framework for building AI applications</li>\n",
        "                    <li><strong>FAISS:</strong> Vector database for similarity search</li>\n",
        "                    <li><strong>Google Gemini AI:</strong> Advanced language model</li>\n",
        "                    <li><strong>HuggingFace Embeddings:</strong> Text embedding generation</li>\n",
        "                    <li><strong>Gradio:</strong> Web interface framework</li>\n",
        "                </ul>\n",
        "\n",
        "                <h3>📋 How It Works</h3>\n",
        "                <ol>\n",
        "                    <li><strong>Upload:</strong> Upload multiple PDF resumes</li>\n",
        "                    <li><strong>Process:</strong> System extracts text and creates vector embeddings</li>\n",
        "                    <li><strong>Query:</strong> Ask for candidates matching specific job profiles</li>\n",
        "                    <li><strong>Results:</strong> AI analyzes and returns best matching candidates</li>\n",
        "                </ol>\n",
        "\n",
        "                <h3>🎯 Use Cases</h3>\n",
        "                <ul>\n",
        "                    <li>HR recruitment and candidate screening</li>\n",
        "                    <li>Talent acquisition for specific roles</li>\n",
        "                    <li>Resume database management</li>\n",
        "                    <li>Quick candidate profiling</li>\n",
        "                </ul>\n",
        "\n",
        "                <h3>⚙️ Setup Requirements</h3>\n",
        "                <p>Make sure you have the following API keys configured:</p>\n",
        "                <ul>\n",
        "                    <li><code>GOOGLE_API_KEY</code> - For Gemini AI</li>\n",
        "                    <li><code>HUGGINGFACEHUB_API_TOKEN</code> - For embeddings</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "        # Event handlers\n",
        "        process_btn.click(\n",
        "            fn=analyzer.process_resumes,\n",
        "            inputs=[file_upload],\n",
        "            outputs=[status_output, msg_input],\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        send_btn.click(\n",
        "            fn=analyzer.chat_with_system,\n",
        "            inputs=[msg_input, chatbot],\n",
        "            outputs=[chatbot, msg_input]\n",
        "        )\n",
        "\n",
        "        msg_input.submit(\n",
        "            fn=analyzer.chat_with_system,\n",
        "            inputs=[msg_input, chatbot],\n",
        "            outputs=[chatbot, msg_input]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the application\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    # Check for required environment variables\n",
        "    required_vars = [\"GOOGLE_API_KEY\", \"HUGGINGFACEHUB_API_TOKEN\"]\n",
        "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "    if missing_vars:\n",
        "        print(f\"⚠️  Missing environment variables: {', '.join(missing_vars)}\")\n",
        "        print(\"Please set these variables in your .env file or environment\")\n",
        "\n",
        "    demo = create_interface()\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=True,\n",
        "        debug=True 'backend', 'fullstack', 'devops', 'data science', 'machine learning',\n",
        "            'project management', 'leadership', 'team', 'work', 'employment',\n",
        "            'education',\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}